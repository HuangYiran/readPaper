{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 本文的主要贡献\n",
    "提供一个新的网络架构：纯attention(包括self attention 和 multi head attention)架构。<br>\n",
    "We propose a new simple network architecture, the Transformer, based solely on attention mechanisms to draw global dependencies between input and output, dispensing with recurrentce and convolutions entirely. The Transformer allows for significantly more parallelization and can be reach a new state of the art in translation quality.<br>\n",
    "self attention: sometimes called intra-attention is an attention mechanism relating different positions of a single sequence in order to compute a representation of the sequence.\n",
    "- RNN的限制： \n",
    "\n",
    "RNN网络的输出$h_t$同时受$h_{t-1}$和$input_t$的影响。This inherently sequential nature precludes parallelization within training examples, which becomes critical at longer sequence lengths, as memory constraints limit batching across examples.<br>\n",
    "这里的前后关系不是很明白，可以理解，他阻碍了并行，但为什么说这点在长句中是致命的？？\n",
    "\n",
    "- CNN的限制\n",
    "\n",
    "同样存在RNN的困扰，不过情节比较轻而已。\n",
    "[22, 17, 9]all of which use convolutional neural networks as basic building block, computing hidden representations in parallel for all input and output positions. In these models, the number of operations required to relate signals from two arbitrary input or output positions grows in the distance between positions, linearly for ConvS2S and logarithmically for ByteNet.\n",
    "- attention的优势\n",
    "\n",
    "可以很好的模拟长句内，不同部分（单词，短语）的依赖关系。<br>\n",
    "The number of operations required to realte signals from two arbitrary input or output positions is reduced to a constant, albeit at the cost of reduced effective resolution due to averaging attention-weighted positions.(multi-head attention)<br>\n",
    "allowing modeling of dependencies without regard to their distance in the input or output sequences.<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 模型架构\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ？？？\n",
    "RNN的限制中的as memory constraints limit batching across example不是很理解？？？\n",
    "Model Architecture中说的competitive neural sequence transduction models是指什么？？为什么这么称呼？？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 还要看的\n",
    "[4] : reading comprehension, self attention<br>\n",
    "[9] : ConvS2S<br>\n",
    "[17]: ByteNet<br>\n",
    "[20]: fractorization tricks<br>\n",
    "[21]: task-independent sentence representations, self attention<br>\n",
    "[22]: extended Neural GPU<br>\n",
    "[26]: abstractive summarization, self attention<br>\n",
    "[27]: textual entailment, self attention<br>\n",
    "[31]: conditional computation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python2-tf",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
