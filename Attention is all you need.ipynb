{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 本文的主要贡献\n",
    "提供一个新的网络架构：纯attention(包括self attention 和 其他 attention)架构。<br>\n",
    "We propose a new simple network architecture, the Transformer, based solely on attention mechanisms to draw global dependencies between input and output, dispensing with recurrentce and convolutions entirely. The Transformer allows for significantly more parallelization and can be reach a new state of the art in translation quality.<br>\n",
    "self attention: sometimes called intra-attention is an attention mechanism relating different positions of a single sequence in order to compute a representation of the sequence.\n",
    "- RNN的限制： \n",
    "\n",
    "RNN网络的输出$h_t$同时受$h_{t-1}$和$input_t$的影响。This inherently sequential nature precludes parallelization within training examples, which becomes critical at longer sequence lengths, as memory constraints limit batching across examples.<br>\n",
    "这里的前后关系不是很明白，可以理解，他阻碍了并行，但为什么说这点在长句中是致命的？？\n",
    "\n",
    "- CNN的限制\n",
    "\n",
    "同样存在RNN的困扰，不过情节比较轻而已。\n",
    "[22, 17, 9]all of which use convolutional neural networks as basic building block, computing hidden representations in parallel for all input and output positions. In these models, the number of operations required to relate signals from two arbitrary input or output positions grows in the distance between positions, linearly for ConvS2S and logarithmically for ByteNet.\n",
    "- attention的优势\n",
    "\n",
    "可以很好的模拟长句内，不同部分（单词，短语）的依赖关系。<br>\n",
    "The number of operations required to realte signals from two arbitrary input or output positions is reduced to a constant, albeit at the cost of reduced effective resolution due to averaging attention-weighted positions.(multi-head attention)<br>\n",
    "allowing modeling of dependencies without regard to their distance in the input or output sequences.<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 模型架构\n",
    "![Transformer](https://raw.githubusercontent.com/HuangYiran/readPaper/master/fotos/Transformer.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Encoder\n",
    "由六个相同的子模型stack而成。每个子模型包括两层，即multi-head attention层和feed forward层（fc）。每个层都加入残缺机制（residual）并于其后进行bn操作。为了方便残缺求和，每层的输出的神经元个数等于输入embedding的长度，这里是512。（每层的输出为：LayerNorm(x + sublayer(x))）\n",
    "#### Decoder\n",
    "由六个相同的子模型stack而成。每个模型包括三层，即masked multi-head attention, multi-head attention层(这回部分输入由encoder获得)和feed forward层（fc）。应该注意的是，目标语言输入都向右平移一个位置。mask就是为了确保，当预测第i个输出的时候，他的信息只来自时间i之前的输出。<br>\n",
    "this masking, combined with fact that the output embeddings are offset by one position, ensures that the predictions for position i can depend only on the known outpus at postions less than i.\n",
    "#### Attention \n",
    "什么是attention。attention可以当成是一个映射方程，他把(query, key, value)作为输入，输出对应的output。其中query, key, value和output都是向量。 output是value的加权求和，权值由query和key决定。<br>\n",
    "An attention function can be described as mapping a query and a set of key-value pairs to an output, where the query, key, values and output are all vectors. Thee output is computed as a weighted sum of the values, where the weight assigned to each value is computed by a compatibility function of the query with the corresponding key.\n",
    "##### Scaled Dot-Product Attention\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ？？？\n",
    "RNN的限制中的as memory constraints limit batching across example不是很理解？？？<br>\n",
    "Model Architecture中说的competitive neural sequence transduction models是指什么？？为什么这么称呼？？<br>\n",
    "Model Architecture中的positional encoding具体不了解？？<br>\n",
    "Model Architecture中的we also modify the self-attention sub-layer in the decoder stack to prevent positions from< attending to subsequent positions.不了解是怎么实现的，也不了解为什么要这么做。<br>\n",
    "为什么decoder的输入都要向右位移一个位置。<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 还要看的\n",
    "[4] : reading comprehension, self attention<br>\n",
    "[9] : ConvS2S, CNN<br>\n",
    "[11]: residual connection\n",
    "[17]: ByteNet, CNN<br>\n",
    "[20]: fractorization tricks<br>\n",
    "[21]: task-independent sentence representations, self attention<br>\n",
    "[22]: extended Neural GPU, CNN<br>\n",
    "[26]: abstractive summarization, self attention<br>\n",
    "[27]: textual entailment, self attention<br>\n",
    "[31]: conditional computation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python2-tf",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
