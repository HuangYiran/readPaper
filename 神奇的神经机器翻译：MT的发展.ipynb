{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "本文大部分来自机器之心"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 1950~1980 MT的思想最招是有Warren Weaver于1949年提出的。在很长一段时间（1950~80），机器翻译是通过研究原语言与目标语言的语言学信息来做的，也就是基于词典和语法生成翻译，这被称为基于规则的机器翻译（RBMT）。\n",
    "- 1980~2000 随着统计学的发展，研究者开始将统计学模型应用于机器翻译，这种方法是基于双语文本语料库的分析来生成翻译结果，这种方法被称为统计机器翻译（SMT）\n",
    "- 1997年Ramon Neco和Mikel Forcada提出使用encoder-decoder结构做机器翻译的想法\n",
    "- 2003年，蒙特利尔大学Yoshua Bengio领导的一个研究团队开发了一个基于神经网络的语言模型，改善了传统SMT模型的数据稀疏性问题。\n",
    "- 2013 Nal Kalchbrenner和Phil Blunsom提出了一种用于机器翻译的新型端到端encoder-decoder结构。该模型可以使用CNN将给定的一段原文本编码成一个连续的向量，然后再使用RNN作为解码器将该状态向量转换成目标语言。他们的研究成果可以说是神经机器翻译NMT的诞生。RNN应该还能得到无限长度句子背后的信息，从而解决所谓的长距离重新排序（long distance reordering）问题。但是，梯度爆炸/消失问题让RNN实际上难以处理长距依存long distance dependence；因此，NMT模型一开始的表现并不好。\n",
    "- 2014 Sutskever et al.和Cho et al.开发了一种名叫seq2seq学习的方法，可以将RNN即用于编码器也可以用于解码器。并且还为NMT引入了LSTM，从而让模型可以远远更好地获取句子中的long distance dependence。但同时将NMT的主要难题变成了了固定长度向量问题：不管源句子的长度几何，中间状态向量的长度是固定的，这回再解码过程中带来更大的复杂性和不确定性，尤其是当源句子很长的时候。\n",
    "- 2014 Yoshua Bengio的团队为NMT引入了attention机制之后，固定长度问题也开始得到了解决"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python2-tf",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
