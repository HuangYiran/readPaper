{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 针对问题\n",
    "Source: Ammo muammolar hali ko’p, deydi amerikalik olim Entoni Fauchi.<br>\n",
    "Reference: But still there are many problems, says American scientist Anthony Fauci.<br>\n",
    "Baseline NMT: But there is still a lot of problems, says James Chan.<br>\n",
    "这里NMT的三个候选是Chan, Fauchi, Jenni。在训练数据中，这三个名字都和免疫学有关。而且Chan出现的频率比Fauchi高，James出现的频率并Anthony或Magaret高。为什么NMT会选择使用Chan呢？？\n",
    "因为NMT学习的是单词在连续空间中的表述。他更加注重的是这个单词在当前语境中是否显得自然，而不那么在乎，在原始句子中的具体内容是什么。这导致的结果是，NMT的译文经常是比较通顺的，但是却缺乏准确度。Because NMT learns word representations in continuous space, it tends to translate words that “seem natural in the context, but do not reflect the content of the source sentence” <br>\n",
    "那么为什么会出现上面这种情况呢？？<br>\n",
    "我们知道，在每个时间节点，输出单词e的概率分布是：<br>\n",
    "$p(e)\\ \\propto\\ exp(W_e*\\hat{h}+b_e)  $<br>\n",
    "其中的W和b仅由具体的单词e决定，h由原句子和上一个单词决定。W和H的点乘代表着，该单词和当前环境的匹配程度.<br>\n",
    "作者认为，<br>\n",
    "- 这个点乘在针对不同出现频率的单词的处理是不均衡的。因固定这两个向量的norm的值为一个常数(这点不是很了，频率和向量norm大小的关系，为什么会有这种关系？？)\n",
    "- 这里与原文的连接不够紧密，从而可能轻视原文的部分内容。作者认为，应该引入新的对象，使得这个对象能够代表原文的信息。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NMT\n",
    "论文第二部分简单的介绍了一下NMT系统，以及他是如何获得对应的单词的。前一部分是简单的encoder，decoder加attention。后一部分在上面已经提到了"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalization\n",
    "解释了单词向量的norm对其概率分布的影响。这个应该没什么好说的。只是，里面强行把词向量的norm大小和词汇出现的频率给联系起来了，可这个应该是embedding的问题吧，对于这点，还是表示不清楚？？？<br>\n",
    "同时这里也给出了改进的方法：<br> \n",
    "- We do this by projected gradient descent: after an update, project each We onto the hypersphere of radius r. \n",
    "- 对h我们做相似的操作，这里我们使用$\\frac{r}{||\\hat{h}||}\\hat{h_t}$代替上面提到的$\\hat{h_t}$\n",
    "\n",
    "下图说明了频率和norm之间的关系，真不知道他是怎么得到的数据，还有他的positive与否怎么感觉就是拿这一个例子来说事，总感觉有那么点不靠谱。\n",
    "![Lexicon choice](https://raw.githubusercontent.com/HuangYiran/readPaper/master/fotos/Lexical_choice.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lexical Translation\n",
    "h不仅受到原文中对应单词的影响，同时还受到对应单词所在的环境，和上一个时间节点的目标单词的上下文的影响。这可能导致生成的单词，更倾向于这些环境，而不是其与原文中对应的单词。与此相反的，基于数量的统计模型就没有这个烦恼，因为它压根就没有考虑到上下文这种东西。Arthur et al. 2016这篇paper中尝试着结合上面提到的这两个家伙，然而效果并不咋滴。<br>\n",
    "这里作者的做法是，使用一个单隐藏层的(带skip的)前向神经网络。该神经网络的输入是原文单词词向量的加权求和，权值由Attention机制中得到。并把输出结合到目标单词的选择之中。<br>\n",
    "$f^l_t = tanh\\sum_s a_t(s)f_s$<br>\n",
    "$h^l_t = tanh(Wf^l_t)+f^l_t$<br>\n",
    "$p(y_t|y_{<t},x) = softmax(W^0\\hat{h_t}+b^0+W^lh^l_t+b^l)$<br>\n",
    "前面提到的normalizition同样也被应用于这里\n",
    "这里的$W^l$是什么东西，表示黑人问号脸？？？？？？？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 后续推荐读物\n",
    "- Koehn and Knowles, 2017 NMT存在的问题"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Py27",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
