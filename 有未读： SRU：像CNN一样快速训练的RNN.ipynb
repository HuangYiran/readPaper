{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "主要内容来自机器之心"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### RNN的瓶颈\n",
    "由于在并行状态计算上的内在困难， RNN的缩放很差。比如，$h_t$的前向计算会一直被阻止，知道$h_{t-1}$的整个计算结束。这对并行计算来说是一个主要的瓶颈。\n",
    "#### 介绍\n",
    "在这次研究中，我们将介绍一种叫简单循环单元SRU的工具，它比起目前出现的循环实现实现都要快得多。循环单元简化了状态计算，从而表现除了类似CNN、注意力模型和前馈网络的相同并行性。特别是，虽然内态$c_t$仍然利用以前的状态$c_{t-1}$更新，但是在循环步骤中，已经不再依赖于$h_{t-1}$了。结果，循环单元中所有的矩阵乘法运算可以很轻易在任何维度和步骤中并行化。与cuCNN和conv2d的实现类似，我们对SRU使用CUDA级别的最优化方法， 将所有元素指向的操作编入一个单一的核函数调用中。\n",
    "#### 实现\n",
    "性能最好的循环神经网络LSTM和门控制循环单元GRU利用神经门来控制信息流并缓解梯度消失于爆炸问题：<br>\n",
    "$c_t = f_t \\odot c_{t-1} + i_t \\odot \\widetilde{x_t}\\ $<br>\n",
    "$= f_t\\odot c_{t-1} + (1-f_t)\\odot \\widetilde{x_t}$<br>\n",
    "$f_t, i_t$是sigmoid门(遗忘门和输入门)。$x_t$为在步骤t转换的输入。我们选择共轭表达式来简化表达$x_t$的计算对于不同的RNN案例来说也不相同。我们使用最简单的形式对输入向量进行线性变换。最后，将内部状态$c_t$传递给激活函数g()以计算输出状态$h_t=g(c_t)$<br>\n",
    "我们再实现中还是用了两个附加特征。首先我们再循环层之间添加了跳过连接，因为他们训练深度网络十分有效(He et al. 2016; Srivastava et al., 2015; Wu et al., 2016a)。具体来说，我们采用了高速连接(highway connection/srivastava et al., 2015), 此外输出状态$h_t'$可以通过以下表达式计算：<br>\n",
    "$h_t' = r_t \\odot h_t + (1-r_t)\\odot x_t$<br>\n",
    "$= r_t \\odot g(c_t) + (1-r_t) \\odot x_t$<br>\n",
    "$r_t$为reset gate。<br>\n",
    "为了RNN正则化我们除了使用标准的dropout外，还使用了变分dropout(variational dropout/Gal and Ghahramani, 2016)。变分dropout在不同的时间步骤t上共享dropout mask。在RNN每个矩阵乘法计算中（即W*drop（$x_t$）），mask需要应用到输入$x_t$。标准的dropout是在$h_t$执行的，即没有馈送到高速连接的输出状态。\n",
    "#### 加速循环\n",
    "现有的RNN实现在循环计算中使用了前面的输出状态$h_{t-1}$。例如，遗忘向量可以通过$f_t = \\sigma(W_f*x_t + R_f*(h_{t-1}+b_f)$计算得出。但是该式中的$R*h_{t-1}$会破坏独立性和并行性，因为隐藏状态每一个维度都依赖于其他状态，因此$h_t$的计算必须等到整个$h_{t-1}$都完成计算。<br>\n",
    "我们提出了完全drop连接，即在$h_{t-1}$和第t步神经们之间的连接。SRU相关联的等式在下面给出：<br>\n",
    "$3)\\ \\widetilde{x_t}=W*x_t$<br>\n",
    "$4)\\ f_t = \\sigma(W_fx_t + b_f)$<br>\n",
    "$5)\\ r_t = \\sigma(W_rx_t + b_f)$<br>\n",
    "$6)\\ c_t = f_t\\odot c_{t-1}+(1-f_t)\\odot \\widetilde{x_t}$<br>\n",
    "$7)\\ h_t = r_t\\odot g(c_t) + (1-r_t)\\odot x_t$<br>\n",
    "对于给定的向量序列{$x_1,..,x_n$}, {$\\widetilde{x_t},f_t,r_t$}对于不同的时间步长t = 1, ...,n是独立的，因此可以并行计算所有这些向量。<br>\n",
    "我们的方法和最近提出的Quasi-RNN(Bradbury et al. 2017)十分相似。当我们再上方3到5表达式中的线性转换项drop $h_{t-1}$时，Quasi-RNN使用k-gram conv2d运算来替代线性项。我们设计出的神经网路的瓶颈在于方程3到5中间的三个矩阵乘法。在计算$\\widetilde{x_t}\\ f_t\\ r_t $后，方程6和7能够非常迅速和简介的执行计算，因为他们的运算都是对应元素之间的操作。<br>\n",
    "#### CUDA优化\n",
    "在现存的深度学习库中，一个简单的SRU实现相比于简单的LSTM实现可快5倍。优化SRU和cuDNN LSTM(Appleyard et al. 2016)相似，但要更加简单一些"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "最好再看一下的：\n",
    "He et al. 2016; Srivastava et al., 2015; Wu et al., 2016a： 跳过连接\n",
    "highway connection/srivastava et al., 2015\n",
    "variational dropout/Gal and Ghahramani, 2016： 变分dropout\n",
    "Quasi-RNN(Bradbury et al. 2017)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python2-tf",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
