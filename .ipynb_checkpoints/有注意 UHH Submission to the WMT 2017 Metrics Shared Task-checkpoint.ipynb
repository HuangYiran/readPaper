{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 简介\n",
    "本文提出了一种基于序列和树核函数(tree kernel function)的评分标准。同时还做了一些其他的研究。<br>\n",
    "相较于前面的研究，本文主要在一下三个方面做了努力：\n",
    "- 我们使用了Partial Tree Kernel(PTK)\n",
    "- 探究了sequence kernels对kernel evaluation metric的影响程度\n",
    "- 探究了, 当把pseudo-reference和back-translations作为kernel function的输入的时候，他们对metric的影响。\n",
    "\n",
    "另外，文中提到了很多相关的作品，其中很多都是值得一看的。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 方法\n",
    "本体提到的metric，我们称之为TSKM，他同时结合PTK和SK，具体如下：\n",
    "- $TSKM_{basic}=TSKM(r,c)=\\frac{PTK(r,c)+SK(r,c)}{2}$\n",
    "同时我们还做了一下的对比项，来确定pseudo-reference和back-translation的作用：\n",
    "- $TSKM_{conb}=\\frac{TSKM_{basic}+TSKM_{pseudo}+TSKM_{back}}{3}$\n",
    "- $TSKM_{pseudo}=TSKM(c, s_t)=\\frac{PTK(c, s_t)+SK(c,s_t)}{2}$\n",
    "- $TSKM_{back}=TSKM(s, c_t)=\\frac{PTK(s,c_t)+SK(s,c_t)}{2}$\n",
    "- 其中r和c分别代表Reference和candidate。$s_t$和$c_t$分别代表pseudo-reference和back-translation\n",
    "\n",
    "测试pseudo-reference主要是为了观察增加相近的Reference，能否优化结果，另外也想尝试实现在没有真正的Reference的情况下，尽可能的优化推测结果。测试train-trainslation的目的纯粹是想看，他能否起到帮助作用。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 值得再看看的\n",
    "1. a syntactic evaluation based on tree kernels(Liu and Gildea 2005)\n",
    "2. MT evaluation that make use of tree kernels(Guzman et al. 2014)\n",
    "3. back-translation(Rapp 2009, Ammar et al. 2013)\n",
    "4. SK(Bunescu Mooney 2005, Nguyen et al. 2006a)\n",
    "5. kernel function(Moschitti 2006a)\n",
    "6. Kerne-based Learning Platform(Filice et al., 2015b, Filice et al. 2015a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 注意\n",
    "1. 能否通过比较原文和Reference之间的关系，来参考那个Reference更适合给候选的译文打分？？？\n",
    "2. 或者通过提取多个不同的reference之间的关联，然后用这些信息与candidate进行对比评分\n",
    "3. 能否把Reference带入原网络把得到的结果来进行评分，这样做有好处吗？？？\n",
    "4. 觉得能从back translation和pseudo reference中得到一些启发\n",
    "\n",
    "5. 能否先用autoencoder的想法，使用所有的reference训练一个autoencoder（或CNN Autoencoder）。然后使用Autoencoder的中间信息作为输入信息，比较得分。在这之前应该了解一下Autoencoder的使用条件。\n",
    "6. 好像一直以来的思路都是去掉Reference中的结构信息，尝试单纯从语义的角度来看候选译文的可靠性。但是译文结构，其实也是人类评价一个译文好坏的重要标准。判断一个候选译文的结构是好是坏，不应该从他与Reference的结构区别来看，而是应该根据他和标准结构的相似度来看。感觉只要结构对了，译文和原文中，相关的单词都出现了，那么即使没有Reference，估计也是可以评判一个译文的好坏的。其实Reference存在意义，比较大程度上是为了验证语义上的相识度。另外由于候选MT是从原文直接翻译过来的，所以我们比较愿意相信，他和原文应该是由相似的结构的（？？）。因此可以有这样的想法，比较候选以译文和原文来确定结构的得分，再通过比较候选和Reference来确定语义的得分，综合上述得分来确定最终得分。\n",
    "7. 存在一个问题，我通过比较候选和原文来得到结构的得分的逻辑是，MT能够从原文中得到结构信息，并把这个信息传递到候选译文上。问题是如果这个成立，那么我这个比较就显得多此一举，但如果这个不成立，那么这个比较就显得没有意义。那我就只能期待说，这个传递的过程是不完整的。从这一点上，来判定这个MT的好坏。但是这样做其实是没有什么道理的，因为感觉结构很大程度上取决于训练时，用的是什么样本。\n",
    "8. 综上，在训练MT的时候，果然还是应该通过调整训练样本来提高MT的质量啊。因为训练样本中原文和译文的结构没办法一一对应的话，那么MT就趋向于，去掉这部分信息。\n",
    "9. 比较可以确定的一点是，把Reference放回MT中，在通过他的中间层输出作为判分标准，思路是可以的，因为从原文到译文结构的转化，在MT中已经被训练从一种内定的格式，所以把不同的结构的译文放到MT中，有一定的概率会得到相同结果的输出，这个不敢肯定，但是可以试一试？？？！！！但同样的，这也是没有办法确定候选译文结构好坏的！！！"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python2-tf",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
