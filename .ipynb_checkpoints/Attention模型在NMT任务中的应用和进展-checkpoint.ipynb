{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "基本是从机器之心中抄过来的，不错的目录和总结。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural Machine Translation by Jointly Learning to Align and Translate \n",
    "- 这篇论文首次提出在NMT中使用attention机制，可以使模型自从确定奇缘句子中和目标词语最相关的部分，\n",
    "- attention计算方法如下: query,key,value, e表示query和key的相似度，h表示value，c表示得分，这里key和value是相同的\n",
    "\n",
    "$c_i = \\sum^{T_x}_{j=1}\\alpha _{ij}h_j$<br>\n",
    "$\\alpha _{ij} = \\frac{exp(e_{ij})}{\\sum^{T_x}_{k=1}exp(e_{ik})}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Effective approaches to attention-based neural machine translation \n",
    "- 问题：对于Attention实现架构的讨论还很少，尤其是全局Attention的计算效率问题。本文就是讨论各种优化策略，包括Global Attention， local Attention, input -feeding方法等\n",
    "- Global Attention，生成上下文向量$c_t$时，考虑原文编码过程中的所有隐状态\n",
    "- Local Attention, 对于每个正在生成的译词，预测一个原文对齐的位置，只考虑该位置前后一个窗口范围内的原文编码隐状态\n",
    "- input feeding, 用一个额外的向量，来记住哪些词已经翻译过了，即考虑coverage的问题。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modeling Coverage for neural machine Translation \n",
    "- 解决经典神经机器翻译模型中存在的over-translation和under-translation的问题\n",
    "- 模型: 在传统NMT模型中，加入统计器翻译策略中的coverage方法，来追踪、判断原始句子是否被翻译。\n",
    "- $C_{i,j} = f(C_{i-1, j},\\alpha_{i,j}, h_j, t_{i-1})$<br>$e_{i,j} = a(t_{i-1}, h_j, C_{i-1, j})=v_a^Ttanh(W_at_{i-1}+U_ah_j+V_aC_{i-1,j})$<br>\n",
    "\n",
    "其中C，为新引入的coverage向量"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Agrrement-based Joint Training for Bidirectional Attention-based Neural Machine Translation 需看原文\n",
    "- 问题：由于自然语言错综复杂的结构，单向的注意力模型只能引入注意力机制的部分regulization。文章提出了联合训练双向的注意力模型，尽可能使注意力在两个方向上保持一致。\n",
    "- 模型：模型的中心思想就是对于相同的training data，使source-to-target和target-to-source两个模型在aligment matrices上保持一致。这样能够去掉一些注意力噪声，使注意力更加集中、准确、更确切地说，作者引入了一个新的目标函数:<br>\n",
    "$J(\\overrightarrow{\\theta},\\overleftarrow{\\theta}) = \\sum_{s=1}^{S}log P(y^{(s)}|x^{(s)};\\overrightarrow{\\theta})$<br>\n",
    "$+\\sum_{s=1}^Slog P(x^{s}|y^{s};\\overleftarrow{\\theta})$<br>\n",
    "$- \\lambda\\sum_{s=1}^S\\Delta(x^{s},y^(s),\\overrightarrow{A}^{s}(\\overrightarrow{A}^(s),\\overleftarrow{A}^{(s)})(\\overleftarrow{\\theta}))$<br>\n",
    "第一项表示对于句子s source-to-target的alignment matrix。最后一项是损失函数，可以含量俩个alignment matrix之间的disagree程度。\n",
    "- 对于，有集中不同的定义方法：Square of addition, Square of subtraction, Multiplication"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Improving Attention Modeling with Implicit Distortion and Fertility for Machine Translation\n",
    "- 问题：使用Attention机制解决NMT中调序和繁衍率的问题\n",
    "- 模型：RecAtt Decoder, 使用之前时刻的context，辅助Attention计算。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
