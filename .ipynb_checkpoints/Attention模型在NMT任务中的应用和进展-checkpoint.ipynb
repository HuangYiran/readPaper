{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "基本是从机器之心中抄过来的，不错的目录和总结。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural Machine Translation by Jointly Learning to Align and Translate \n",
    "- 这篇论文首次提出在NMT中使用attention机制，可以使模型自从确定奇缘句子中和目标词语最相关的部分，\n",
    "- attention计算方法如下: query,key,value, e表示query和key的相似度，h表示value，c表示得分，这里key和value是相同的\n",
    "\n",
    "$c_i = \\sum^{T_x}_{j=1}\\alpha _{ij}h_j$<br>\n",
    "$\\alpha _{ij} = \\frac{exp(e_{ij})}{\\sum^{T_x}_{k=1}exp(e_{ik})}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Effective approaches to attention-based neural machine translation \n",
    "- 问题：对于Attention实现架构的讨论还很少，尤其是全局Attention的计算效率问题。本文就是讨论各种优化策略，包括Global Attention， local Attention, input -feeding方法等\n",
    "- Global Attention，生成上下文向量$c_t$时，考虑原文编码过程中的所有隐状态\n",
    "- Local Attention, 对于每个正在生成的译词，预测一个原文对齐的位置，只考虑该位置前后一个窗口范围内的原文编码隐状态\n",
    "- input feeding, 用一个额外的向量，来记住那些词已经翻译过了，即考虑coverage的问题。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modeling Coverage for neural machine Translation \n",
    "- 解决经典神经机器翻译模型中存在的over-translation和under-translation的问题\n",
    "- 模型: 在"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
