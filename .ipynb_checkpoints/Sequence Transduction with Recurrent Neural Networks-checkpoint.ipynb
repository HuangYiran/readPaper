{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Motivation\n",
    "序列转换的一大难点是：寻找一种表达输入和输出序列的方法，并且这种方法能够不受原序列形变的影响。RNN被证明是一种很好的方法。但是RNN还是存在一个问题。\n",
    "One of the key challenges in sequence transduction is learning to represent both the input and output sequences in a way that is invariant to sequential distortions sch as shrinking, stretching and translating.<br>\n",
    "\n",
    "在使用传统的RNN之前，需要定义输入序列和输出序列中各个项的对应关系。而这时一个很困难的问题。\n",
    "RNNs traditionally require a pre-defined alignment between the input and output sequences to perform transdution.\n",
    "This is a severe limitation since finding the alignment is the most difficult aspect of many sequence transduction problems. Indeed, even determining the length of the output sequence is often challenging.<br>\n",
    "\n",
    "这份paper介绍了一种，端对端的，完全基于RNN的概率序列转换系统。并且它能够很好的处理上面提到的问题。它：\n",
    "- extends CTC(CTC的输出必须小于输入) by defining a distribution over output sequences of all lengths, and by jointly modelling both input-output and output-output  dependencies."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "补：<br>\n",
    "RNN: The combination of a high-dimensional multivariate internal state and nonlinear state-state dynamics offers more expressive power than conventional sequential algorithms such as hidden Markov models.<br>\n",
    "However RNNs are usually restricted to problems where the alignment between the input and output sequence is known in advance. <br>\n",
    "If the network outputs are probabilistic this leads to a distribution over touput  sequence of the same length as the input sequence. But for a general purpose sequence transducer, where the output length is unknown in advance, we would prefer a distribution over sequences of all lengths."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recurrent Neural Network Transducer\n",
    "- 和CTC一样，增加一个空标签\n",
    "- 设a为一个种alignment的方法，即CTC中的path。给定一个x，RNN转换器输出对应的条件分布$Pr(a\\in y^*|x))$\n",
    "- 为了得到上面说的条件分布，我们再这里使用了两个神经网络：transcription network, prediction network。\n",
    "    - transcription network扫描输入的序列x(用biRNN)，并输出状态序列f=($f_1, ...,f_T$)(为了简化，使得序列的长度和输入序列的相等)。The transcription network is similiar to a Connectionst Temporal Classification RNN, which also uses a null output to define a distribution over input output  alignments.\n",
    "    - prediction network是一个预测网路，他使用RNN网络，输入节点的个数有label空间的大小决定（因为用one hot），输出节点数为label孔家的打小加一。他通过扫描目标序列，输出预测序列。 referred to as the prediction network G, sacns the output sequence y and output the prediction vector sequence.\n",
    "- 已知transcription vector $f_t$, where 1<= t<= T, the prediction vector $g_u$, where 0<= u<= U, and label k$\\in$ y, define the output density distribution:$h(k, t, u)=exp(f^k_t+g^k_u)$，其中k指定序列中的第k个元素(不明白k的作用？？？，应该是指对应的操作才对啊？？？)。对应的output distribution是: $Pr(k \\in y|t, u) = \\frac{h(k, t, u)}{\\sum_{k'\\in y}h(k', t, u)}$。<br>$Pr(k|t,u)$表示在对应的位置，发生转移的可能性\n",
    "- 为了方便，我们做一下标识，使得：\n",
    "    - $y(t,u)=Pr(y_{y+1}|t, u)$\n",
    "    - $\\phi(t,u) = Pr(\\phi|t, u)$\n",
    "- 通过下图可以更好的理解上面的定义：\n",
    "![sequence transduction](https://raw.githubusercontent.com/HuangYiran/readPaper/master/fotos/Sequence_transduction.png)\n",
    "- 使用Forward-Backward algorithm: \n",
    "    - 定义forward Variable $\\alpha(t,u)$:指定当出现输入$f_{[1:t]}$时，输出$y_{[1:u]}$的概率(as the probabilitz of outputting y during f)。可以通过下面的式子进行计算：<br>\n",
    "    a(t,u) = a(t-1, u)$\\phi(t-1, u)+\\alpha(t, u-1)y(t, u-1)$, 初始状态：$\\alpha(1,0)=1$。=>Pr(y|x)=$\\alpha(T, U)\\phi(T, U)$\n",
    "    - 定义backward variable $\\beta(t, u)$:指定当出现$f_{[t, u]}$时，输出$y_{[u+1:U]}$的概率。可以通过下面的式子进行计算：<br>\n",
    "    $\\beta(t, u) = \\beta(t+1, u)\\phi(t, u)+\\beta(t, u+1)y(t,u)$, 使用初始状态$\\beta(T,U)=\\phi(T,U)$\n",
    "    - From the definition of the forward and backward variable it follows that their product $\\alpha(t, u )\\beta(t, u)$ at any point (t, u) in the output lattice is equal to the probability of emitting the complete output sequence if $y_u$ is emitted during transcription step t."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training\n",
    "给定输入序列x和目标序列$y_*$使用loss方程L=$-ln\\ Pr(y^*|x)$，通过backpropagation调整参数(We do this by calculating the gradient of L with respect to the network weights parameters and performing gradient descent.)<br>\n",
    "Analysing the diffusion of probability through the output lattice shows that shows that Pr($y^*|x)$ is equal to the sum of $\\alpha(t,u)\\beta(t,u)$ over any top-left to bottom right diagonal through the nodes.That is, $\\all n:1<=n<U+T$\n",
    "![training](https://raw.githubusercontent.com/HuangYiran/readPaper/master/fotos/training.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![sequence_transduction](https://raw.githubusercontent.com/HuangYiran/readPaper/master/fotos/sequence_transduction.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## $\\alpha \\beta$是不是可以使用rnn来求？？？？\n",
    "## 不是很了$\\alpha*\\beta$的具体意义？？？？"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python2-tf",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
