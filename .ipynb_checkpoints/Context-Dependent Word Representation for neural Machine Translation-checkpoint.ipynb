{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "本文内容主要来自:\n",
    "- https://arxiv.org/pdf/1607.00578.pdf\n",
    "\n",
    "本文主要内容包括contextualization和symbolization两部分，目前先完成第一部分。剩下的找时间再看吧"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## paper简介\n",
    "我们发现，在机器翻译中使用连续词向量表达符号存在一个潜在的弱点:<br>\n",
    "- The continuous vector representation, or a word embedding vector, of a symbol encodes multiple dimensions of similarity, equivalent to encoding more than one meaning of the word. 也就是说，我们使用一个连续词向量来描述一个单词的多个意思（一个单词和其他单词在不同方面的相识度）。这导致，在机器翻译的过程中，他将根据原文句子提供的上下文花费大量的精力来处理原文和目标文单词之间的歧义。<br>\n",
    "\n",
    "基于这个发现，我们在这里提出:\n",
    "- contextualize the word embedding vectors using a nonlinear bag-of-words representation of the source sentence.\n",
    "- to represent special tokens(such as numbers, proper nouns and acronyms) with typed symbols to facilitate translating those words that are not well-suited to be translated via continuous vectors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 方法简介\n",
    "一般的情况下，神经机器翻译的第一个步骤就是把原始的符号转换成对应的高维连续向量（词向量）。而在这个转换过程中，每个单词是独立完成转换的，也就是说每个单词并不受所在句子的上下文影响。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Contextualized Word Embedding Vectors\n",
    "##### Multiple Dimensions of Similarity\n",
    "高维词向量的一个重要的特征就是: it encodes multiple dimensions of similarities. 也就是说它对多维相似度进行编码（也就是说，他同事对一个单词的多个意思进行编码）。这有利于神经元网络处理一词多义的问题。上面这种现象，我们可以通过下面这个方法观察到：<br>\n",
    "给定高维词向量x'，我们找到离这个词向量最近的N-1个词向量$x^1,...,x^{N-1}$。使用PCA进行降维，这是我们可以观察不同方向下的最近邻。可以发现其结果是不同的，从而证实上面的猜想。\n",
    "![multi dimension similarity](https://raw.githubusercontent.com/HuangYiran/readPaper/master/fotos/multi_dims_similarity.png)\n",
    "##### Contextualized Word Embeddings\n",
    "multiple dimensions of Similarity的问题意味着，神经元网络需要根据单词所在的上下文信息，来消除这个该单词词向量的二义性。在机器翻译中，原文和目标文的单词是分开处理的。在encoder中，一个单词，可以通过所在句子的其他单词，来消除他的二义性。而对于decoder来说，他可以通过上一个单词，或者是原文的内容来做这个处理。<br>\n",
    "正如Weaver在1949年所说的那样，一个单词的二义性，大部分可以通过其周围的单词来消除。对于机器翻译的问题，这意味着，不管是encoder还是decoder，对于每个单词，他需要记住其前面出现的单词，从而来消除自身的二义性。但这并不是我们想要的。我们希望他能够集中精力与它的翻译工作？？<br>\n",
    "为了解决这个问题，我们建议在把词向量扔进RNN之前，进行如下图中的处理：\n",
    "![contextualization](https://raw.githubusercontent.com/HuangYiran/readPaper/master/fotos/contextualization.png)\n",
    "这个方法的中心思想是，根据上下文，通过掩盖掉词向量的一些维度来实现消除二义性的效果。具体过程如下：\n",
    "- 我们用$c^x$来代表原文句子的上下文，他由句子中各个单词通过非线性转换后的求平均而得：$c^x=\\frac{1}{T}\\sum^T_{t=1}NN_\\theta(x_t)$其中NN为一个前向非线性神经网络。\n",
    "- 随后，我们根据这个$c^x$，为每个词向量计算一个mask。并在这个词向量送入rnn之前，对其进行处理：$x_t<-x_t\\gitodot \\sigma(W_xc^x+b_x)$,$y_t<-y_t\\bigodot \\sigma(W_yc^x+b_y)$。其中$\\sigma$用于生成mask。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 存在问题的句子\n",
    "1-5 We conjecture that only a few axes in this high-dimensional space are relevant given a source sentence and that we can remove much of the ambiguity in the choice of words by restricting, or turning off, most of the irrelevant dimensions. <br>\n",
    "3-2 we can qualitatively check this phenomenon of multiple dimensions of similarities by inspecting a local chart of the manifold on which the word embedding vectors reside\n",
    "3-3 In other words, the encoder and decoder must sacrifice their capacity in disambiguating the words. This is undesirable, as what we truly want the encoder and decoder to do is to capture the higher level compositional structures of a sentence that are necessary for translation.??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Py27",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
