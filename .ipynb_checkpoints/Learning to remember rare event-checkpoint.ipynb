{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "本文主要来自：\n",
    "- https://arxiv.org/pdf/1703.03129.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 摘要\n",
    "我们为深度学习提出一个大小可伸缩的(large scale)，生命周期很长的(life-long)记忆模型。为了提高效率这个模型利用了fast nearest-neighbor算法(and thus scales to large memory sizes??)。在这个模型中，除了nearest neighbor外，其他的部分都是可微分的并可以通过end-to-end的形式进行训练。另外提一下，life-long在这里指的是在训练的过程中，不用进行重新设置。<br>\n",
    "这里提到的记忆模型可以很容易的被应用到不同的有监督的神经网络中，为了证明这点，我们做了很多的实验。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 介绍\n",
    "现在的机器学习系统被应用到很多方面，并取得不错的成就。一个好的NMT系统，针对特定的语言对，大体上可以达到人类翻译的水平。但是这里应该强调的是“大体上”这个词。这对在训练集中多次出现的词汇，NMT可以很好的进行翻译，但是与之相反的，如果一个词汇在训练集中出现的次数很少，那么，由于在训练过程中没有得到足够的学习，他的翻译就不是那么可观了。<br>\n",
    "这体现了当今深度学习模型的一个问题：为了能够学习这些稀有事件，我们必须扩展训练集或者进行重复训练。而与之相反的，我们人类这对一个事件要做到长久的记忆，其实只需要一个例子。<br>\n",
    "这对这个问题，我们提出了一个life-long记忆模型，他可以被应用于各种神经网络中，并实现通过少数的例子就能形成长久的记忆。我们的记忆模型有key-value对组合而成。这里的key对应神经网络中选中的层所进行的活动，value对应给定例子的目标(Keys are activations of a chosen layer of a neural network, and values are the ground-truth targets for the given example.)。 通过这种方法，当网络在训练的时候，这个模型会不断的增大，从而提高他的性能。最终，他能实现，通过前面出现的相识的活动，给出预测的翻译。当得到一个新的例子的时候，网络就会把这个例子写到记忆中，并在以后需要的时候会被用到。<br>\n",
    "文中还提到了一些验证的方法，有兴趣的可以看一下。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 记忆模型\n",
    "我们的记忆模型由下面几个部分组成：\n",
    "- 矩阵K，用于存储记忆的keys。每个key是一个key_size长的向量\n",
    "- 向量V，用于存储记忆的values。每个value对应一个整数值。用于指代一个类或者字符ID。\n",
    "- 向量A，that tranks the age of items stored in memory（这个age是指什么，有点迷糊？？）\n",
    "\n",
    "我们用下面的表达方式定义一个memory-size大的记忆:\n",
    "    $M = (K_{memory-size * key-size}, V_{memory-size}, A_{memory-size})$<br>\n",
    "在进行记忆查询的时候，对于查询向量q，我们期望，他能进行过规范化，也就是说$||q|| = 1$。给定这么一个q，我们定义，q在M中的最近邻与q进行点乘后拥有最大的输出。$NN(q, M)= argmax_i\\ q\\cdot K[i]$。<br>\n",
    "因为q进行过个规范化，所以上式，相当于计算他们的cos相似度。我们把上面的式子扩张到k近邻，并用$NN_k(q,M)$表示。在实验中，我们的k进场取值为256.<br>\n",
    "在得定一个查询q的时候，我们会得到k个最近邻$(n_1,..,n_k)$（按相似度进行排序），得到他们对应的value K[$n_i$]，并计算q和这些values的cos相似度，得到对应softmax的结果$(d_1*t,...,d_k*t)$。（The parameter t denotes the inverse of softmax temperature and we set it to t = 40 in our experiments？？？t的作用不是很了。）然后，我们把上面得到的两个家伙进行点乘，得到的结果进行返回。<br>\n",
    "下面介绍一下，如何训练这个记忆模型\n",
    "\n",
    "![memory loss](https://raw.githubusercontent.com/HuangYiran/readPaper/master/fotos/memory_loss.png)\n",
    "##### memory loss 加星，好像说这种方法挺常见的？？\n",
    "假设给定查询q，对应的真实的value是v。如果v出现在我们找到的k近邻中。首先拿我们找出q的k个最近邻$n_1,...,n_k$。我们定义p为使得V[$n_k$]=v的最小的索引，而b为使得V[$n_i$]!=v的最小的索引，并分别称之为positive neighbor和negative neighbor。当v出现在k近邻的时候，我们直接最大化和positive neighbor的相似度，最小化和Negative neighbor的相似度。而当v不在我们找到k近邻的时候，我们需要找到记忆M中值为v的key，并用它来代替K[$n_p$]。综上，我们定义loss为：$loss(q,v,M)=[q\\cdot K[n_b] - q\\cdot K[]n_q] + \\alpha]_+$\n",
    "##### memory update\n",
    "根据模型是否已经返回正确的value，存在两种更新的方法。从图中可以看到对应的方法。\n",
    "##### 如何高效的获得k最近邻\n",
    "跳了，有兴趣再去看"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 如何使用记忆\n",
    "![how to use the memory in NMT](https://raw.githubusercontent.com/HuangYiran/readPaper/master/fotos/memory.png)\n",
    "![how to use the memory in GPU](https://raw.githubusercontent.com/HuangYiran/readPaper/master/fotos/memory2.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Py27",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
